# MLP

Здесь будет рассказано о том, как работает персептрон.     
     
Персептрон, как и любая другая нейронная сеть, выполняет задачу классификации (на самом деле, есть ещё другие примененя, например кластеризация, но сейчас мы поговорим только про классификацию).     
Это значит, что имеется несколько объектов и нужно сказать, к какой группе они принадлежат.     
Примеры:     
+ определить пол человека по фото (вход - ихображение, классы - мужской/женский)
+ определить, является ли ссылка вредоносной (вход - набор слов, из которых состоит ссылка, классы - опасная/безопасная)
+ распознование речи (вход - оцифрованный звук, классы - все доступные слова)     
_____
# Набор данных и оцифровка объектов
В этих всех примерах сеть получала что-то на вход и говорила (предсказывала), к какому конкретно классу принадлежит объект.     
Но персептрон - это математическая модель, поэтому на входе и на выходе должны быть числа. Это сделать очень просто.     
     
Вход (input):     
+ Изображения можно представить в виде пикселей, а каждый пиксель - это три числа (три цветовых канала: r, g, b). Слова можно преобразовать в числа, используя [TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF). Для работы со звуком его можно преобразовать, используя [ряды Фурье](https://ru.wikipedia.org/wiki/%D0%A0%D1%8F%D0%B4_%D0%A4%D1%83%D1%80%D1%8C%D0%B5).    
     
Результат (output):
+ Классы тоже нужно превратить в числа. Если класса два, то можно сказать **0** - это один класс, **1** - это другой. т.е. *0,5* - это максимальная неуверенность в предсказании. Если *0,6*, то это больше правый класс, если *0,2*, то левый.     
+ Но если классов больше двух? Кажется, можно сделать **0** - первый, **1** - второй, **2** - третий. Но это плохая идея, потому что в прошлом примере неуверенность между 0 и 1 - это было *0,5*, а как сделать здесь неуверенность между классами **0** и **2**? *1*? Но разве это не будет значить, что это класс **1**?     
Поэтому для большего количества классов вводится такая схема: результатом будет мтолько чисел, сколько имеется классов (если классов 5, то и чисел на выходе будет 5). Каждое число от *0* до *1*. И это число показывает "уверенность" персептрона, что объект принаждлежит каждому классу. *0* - это точно **не этот** класс, *1* - это точно **этот** класс. *0,5* значит неуверенность.    

Пример: если на у нас есть 3 класса, а выход такой *[0.1, 0.5, 0.87]*, то это значит, что персептрон отнёс наш объект по классам следующим образом:     
+ про первый класс он думает, что это почти точно нет
+ про второй он не уверен
+ про третий он почти уверен, что это правильный класс.     
     
Но для примера рассмотрим что-то более простое. Для введения в нейронные сети используется набор данных "[ирисы Фишера](https://ru.wikipedia.org/wiki/%D0%98%D1%80%D0%B8%D1%81%D1%8B_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0)".     
Этот набор данных представляет из себя 150 записей о трёх видах ирисов (50 на каждый).
+ Каждый отдельный цветок имеет свои размеры. Поэтому для превращения цветка в числа используется длина и ширина лепестков (их два, поэтому каждый цветок представлен четырьмя числами).     
+ Классами являются три сорта ириов. После оцифровки они превратятся в три числа, обозначающие, насколько персептрон уверен в каждом из классов.
Итого имеем, что на вход персептрона подаются 4 числа, а на выходе три числа от 0 но 1.
_____
# Работа персептрона

Любой персептрон - это набор матриц (у простейшего она одна) и функция активации.     
Любой персептрон работает следующим образом:
+ У нас имеются оцифрованные данные (массив/вектор).
+ Он умножается на матрицу.
+ Далее для каждого полученного элемента вызывается функция активации.
+ Продолжать до тех пор, пока матрицы не закончатся.
     
Если таких матриц больше одной, то персептрон называется многослойным (MLP).
     
Сам по себе персептрон не умеет обучаться. Он просто получает что-то на вход, а далее выдаёт какой-то результат. Он может быть совершенно неадекватным. Но нам нужно, чтобы он точно показывал класс объекта. Для этого мы должны изменить те самые матрицы (а точнее числа в них).     
Процесс подбора этих чисел и является обучением персептрона (или обучением нейронной сети, если рассуждать в широком смысле).     
_____
# Обучение персептрона

Для работы персептрона используется input, а для обучения уже важен output (напомню, они оба находятся в датасете).     
     
Есть несколько способов обучать персептрон.     
+ Первый - это ***перебор*** разных значений матрицы, чтобы найти более-менее подходящие. Чтобы понять, какие веса лучше, можно ввести среднюю или средне-квадратичную ошибку. Если ошибка уменьшилась, значит новые веса лучше справляются с задачей. И так до тех пор, пока ошибка не станет достаточно маленькой.
+ Второй способ более культурный - это ***обратное распространение ошибки***. Его суть в том, что...     
У нас есть результат работы персептрона. Например *[0.4, 0.5, 0.7]*, а в датасете сказано, что это объект первого класса. Т.е. мы хотели бы увидеть *[1, 0, 0]*.     
И этот метод меняет веса так, чтобы они стали показывать правильный результат. Т.е. после обучения веса, отвечающие за первый класс будут показывать большее значение (чтобы приблизиться к *1*), а остальные будут давать меньшие значения (чтобы приблизиться к *0*).
